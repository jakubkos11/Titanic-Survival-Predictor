{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3136,"databundleVersionId":26502,"sourceType":"competition"}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.metrics import accuracy_score\n\nclass LogisticRegressionModel:\n    \"\"\"Implements logistic regression model from scratch.\"\"\"\n    def __init__(self, features, iterations, learning_rate, lambda_param):\n        \"\"\"Initialize model.\"\"\"\n        self.features = features\n        self.learning_rate = learning_rate\n        self.iterations = iterations\n        self.lambda_param = lambda_param\n        self.weights = None\n        self.preprocessor = None\n        self.threshold = 0.5\n\n    def _clean_data(self, data):\n        \"\"\"Fill the missing values in columns.\"\"\"\n        df = data.copy()\n        # Note: We dont fill Cabin, because it is late used in IsCabin feature\n        \n        # Fill with the median age grouped by Pclass and Sex for more accuracy.\n        df['Age'] = df.groupby(['Pclass', 'Sex'])['Age'].transform(lambda x: x.fillna(x.median()))\n    \n        # In case any group was entirely NaN, fill remaining with the global median\n        if df['Age'].isnull().any():\n            df['Age'] = df['Age'].fillna(df['Age'].median())\n    \n        # Fill missing values with the median.\n        df['Fare'] = df['Fare'].fillna(df['Fare'].median())\n    \n        # Fill the missing values with the most common port.\n        df['Embarked'] = df['Embarked'].fillna(df['Embarked'].mode()[0])\n\n        return df\n        \n    def _apply_feature_engineering(self, data):\n        \"\"\"Add custom engineering features to improve predictive power of model.\"\"\"\n        df = data.copy()\n        df['FamilySize'] = df['Parch'] + df['SibSp'] + 1\n        df['IsAlone'] = (df['FamilySize'] == 1).astype(int)\n        df['Title'] = data['Name'].str.extract(' ([A-Za-z]+)\\.', expand = False)\n        \n        # Simplify titles into a few main categories\n        common_titles = ['Mr', 'Miss', 'Mrs', 'Master']\n        df['Title'] = df['Title'].apply(lambda x: x if x in common_titles else 'Other')\n\n        df['Deck'] = df['Cabin'].str[0].fillna('M')\n        df['AgeBin'] = pd.cut(df['Age'], bins = [0, 18, 60, 100], labels = ['Child', 'Adult', 'Senior'])\n        df['FareBin'] = pd.qcut(df['Fare'], 4, labels = ['Low', 'Medium', 'High', 'VeryHigh'])\n        df['Sex_Pclass'] = df['Sex'] + '_' + df['Pclass'].astype(str)\n        return df\n\n    def _build_preprocessor(self, X):\n        \"\"\"Builds preprocessing pipeline for numeric and categorical data.\"\"\"\n        # Select only the features we intend to use in the model\n        X_subset = X[self.features]\n\n        # Automatically find numeric and categorical columns based on their data types\n        num_features = X_subset.select_dtypes(include = np.number).columns.tolist()\n        cat_features = X_subset.select_dtypes(include = ['object', 'category']).columns.tolist()\n        \n        num_pipeline = Pipeline([\n            ('imputer', SimpleImputer(strategy = 'median')),\n            ('scaler', StandardScaler())\n        ])\n        cat_pipeline = Pipeline([\n            ('imputer', SimpleImputer(strategy = 'most_frequent')),\n            ('onehot', OneHotEncoder(handle_unknown = 'ignore', sparse_output = False))\n        ])\n        preprocessor = ColumnTransformer([\n            ('num', num_pipeline, num_features),\n            ('cat', cat_pipeline, cat_features)\n        ])\n        return preprocessor\n\n    def _add_bias(self, X):\n        \"\"\"\n        # Adds a bias term to the feature matrix.\n        This is a critical addition for a correct logistic regression implementation.\n        \"\"\"\n        return np.c_[np.ones((X.shape[0], 1)), X]\n        \n    @staticmethod\n    def _sigmoid(z):\n        \"\"\"Computes the sigmoid activation function used in logistic regression.\"\"\"\n        return 1 / (1 + np.exp(-z))\n\n    def _generate_probabilities(self, X):\n        \"\"\"Returns predicted probabilities for binary classification.\"\"\"\n        z = np.dot(X, self.weights)\n        return self._sigmoid(z)\n\n    def train(self, X, y):\n        \"\"\"Trains a logistic regression model using gradient descent.\"\"\"\n        # Build the preprocessor\n        self.preprocessor = self._build_preprocessor(X)\n    \n        # Preprocess the data\n        X = self.preprocessor.fit_transform(X[self.features])\n            \n        # Split data for fair evaluation\n        X_train, X_val, y_train, y_val = train_test_split(\n            X, y, test_size = 0.2, random_state = 1)\n\n        # Add bias\n        X_train = self._add_bias(X_train)\n        X_val = self._add_bias(X_val)\n\n        # Train model using gradient descent on training set\n        self.weights = np.zeros(X_train.shape[1])\n        for i in range(self.iterations):\n            predictions = self._generate_probabilities(X_train)\n            error = predictions - y_train\n            gradient = np.dot(X_train.T, error) / len(y_train)\n\n            # Add penalty to the gradient\n            regularization = (self.lambda_param / len(y_train)) * self.weights\n            regularization[0] = 0 # Do not regularize the bias term\n            gradient += regularization\n            self.weights -= self.learning_rate * gradient\n\n        val_probs = self._generate_probabilities(X_val)\n        self.threshold = self._find_best_threshold(y_val.values, val_probs)\n        return self._evaluate(X_train, y_train, X_val, y_val)\n\n    def _find_best_threshold(self, y_true, y_pred):\n        \"\"\"Finds optimal threshold that maximizes accuracy.\"\"\"\n        best_accuracy = 0\n        best_threshold = 0.5\n        for threshold in np.arange(0.1, 0.9, 0.01):\n            predictions = (y_pred >= threshold).astype(int)\n            accuracy = accuracy_score(y_true, predictions)\n            if accuracy > best_accuracy:\n                best_accuracy = accuracy\n                best_threshold = threshold\n        return best_threshold\n\n    def _evaluate(self, X_train, y_train, X_val, y_val):\n        \"\"\"Prints model accuracy on the training and validation sets.\"\"\"\n        train_probabilities = self._generate_probabilities(X_train)\n        train_preds = (train_probabilities >= self.threshold).astype(int)\n        train_accuracy = accuracy_score(y_train, train_preds)\n\n        val_probabilities = self._generate_probabilities(X_val)\n        val_preds = (val_probabilities >= self.threshold).astype(int)\n        val_accuracy = accuracy_score(y_val, val_preds)\n        \n        print(\"\\n--- Model Evaluation ---\")\n        print(f\"Optimal Threshold: {self.threshold:.2f}\")\n        print(f\"Training Accuracy: {train_accuracy:.4f}\")\n        print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n    \n    def predict(self, X):\n        \"\"\"Gives final predictions for test data.\"\"\"\n        X = self.preprocessor.transform(X[self.features])\n        X = self._add_bias(X)\n        probabilities = self._generate_probabilities(X)\n        return (probabilities >= self.threshold).astype(int)\n\n    def run(self):\n        \"\"\"Runs the entire pipeline from loading data to submission.\"\"\"\n        # Load data\n        train_df = pd.read_csv('/kaggle/input/titanic/train.csv')\n        test_df = pd.read_csv('/kaggle/input/titanic/test.csv')\n\n        # Separate features and target\n        X_train = train_df.drop('Survived', axis = 1)\n        y_train = train_df['Survived']\n        X_test = test_df.copy()\n\n        # Clean data\n        X_train = self._clean_data(X_train)\n        X_test = self._clean_data(X_test)\n\n        # Apply feature engineering\n        X_train = self._apply_feature_engineering(X_train)\n        X_test = self._apply_feature_engineering(X_test)\n\n        # Train model\n        self.train(X_train, y_train)\n\n        # Make predictions on the test set\n        predictions = self.predict(X_test)\n        \n        # Generate the submission file\n        submission = pd.DataFrame({'PassengerId': test_df['PassengerId'], 'Survived': predictions})\n        submission.to_csv('submission.csv', index = False)\n\n# Main function\nif __name__ == '__main__':\n    features = ['Pclass', 'Sex', 'FamilySize', 'IsAlone', 'Title', 'AgeBin', 'FareBin']\n    iterations = 20000\n    learning_rate = 0.05\n    lambda_param = 0.5\n    predictor = LogisticRegressionModel(features, iterations, learning_rate, lambda_param)\n    predictor.run()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-22T18:15:06.045052Z","iopub.execute_input":"2025-09-22T18:15:06.045434Z","iopub.status.idle":"2025-09-22T18:15:10.405256Z","shell.execute_reply.started":"2025-09-22T18:15:06.045410Z","shell.execute_reply":"2025-09-22T18:15:10.404325Z"}},"outputs":[{"name":"stdout","text":"\n--- Model Evaluation ---\nOptimal Threshold: 0.38\nTraining Accuracy: 0.8132\nValidation Accuracy: 0.8101\n","output_type":"stream"}],"execution_count":3}]}