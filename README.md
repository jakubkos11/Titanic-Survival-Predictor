# Titanic: Survival Analysis and Prediction

This project is a comprehensive solution for the Kaggle competition "Titanic: Machine Learning from Disaster". The main goal is to build a predictive model that determines whether a passenger survived the Titanic disaster based on their personal data.

## ğŸ¯ Project Goal

* To perform in-depth **Exploratory Data Analysis (EDA)** to understand the relationships between features and survival.
* To **preprocess the data and perform feature engineering**, including handling missing values and creating new features.
* To train and **compare several classification models**.
* To select the best-performing model and evaluate its effectiveness.

## ğŸ’¾ Dataset

The data comes directly from the Kaggle competition and is available [at this link](https://www.kaggle.com/competitions/titanic). The dataset contains information about passengers, such as age, sex, ticket class, etc.

# Titanic: Survival Analysis and Prediction

This project is a comprehensive solution for the Kaggle competition "Titanic: Machine Learning from Disaster". The main goal is to build a predictive model that determines whether a passenger survived the Titanic disaster based on their personal data.

## ğŸ¯ Project Goal

* To perform in-depth **Exploratory Data Analysis (EDA)** to understand the relationships between features and survival.
* To **preprocess the data and perform feature engineering**, including handling missing values and creating new features.
* To train and **compare several classification models**.
* To select the best-performing model and evaluate its effectiveness.

## ğŸ’¾ Dataset

The data comes directly from the Kaggle competition and is available [at this link](https://www.kaggle.com/competitions/titanic). The dataset contains information about passengers, such as age, sex, ticket class, etc.

## ğŸ“ Project Structure

The repository is organized as follows:

```
â”œâ”€â”€ data/                  # Raw train.csv and test.csv files
â”œâ”€â”€ logistic-regression-model/ # Saved logistic regression model
â”œâ”€â”€ naive-model/           # Saved Gaussian Naive Bayes model
â”œâ”€â”€ random-forest-model/   # Saved Random Forest model
â”œâ”€â”€ XGBoost-model/         # Saved, best-performing XGBoost model
â”œâ”€â”€ requirements.txt       # List of Python dependencies to install
â””â”€â”€ visualizations.ipynb   # Jupyter Notebook with data analysis, visualizations, and the modeling process
```

## âš™ï¸ Methodology

The problem-solving process was divided into several key stages:

1.  **Exploratory Data Analysis (EDA):** The analysis was conducted in `visualizations.ipynb`. It involved examining the distributions of variables and their correlation with the survival rate.
2.  **Data Preprocessing & Feature Engineering:** New features such as `FamilySize` and `IsAlone` were created. Missing values in the `Age`, `Fare`, and `Embarked` columns were handled.
3.  **Model Training:** Four different classification models were trained and evaluated to find the most effective algorithm for this dataset.
4.  **Evaluation:** Models were compared based on the **accuracy** metric on the validation set.

## ğŸ¤– Models Used

The following algorithms were tested as part of this project:
* Logistic Regression
* Gaussian Naive Bayes
* Random Forest
* **XGBoost (eXtreme Gradient Boosting)**

## ğŸ“Š Results

A comparison of the models' performance on the test set on Kaggle is presented below:

| Model                 | Accuracy |
| :-------------------- | :------: |
| Logistic Regression   |   ~79%   |
| Naive Model           |   ~77%   |
| Random Forest         |   ~77%   |
| XGBoost               |   ~76%   |

## ğŸš€ How to Run

1.  Clone the repository:
    ```bash
    git clone [https://github.com/jakubkos11/titanic-predictor.git](https://github.com/jakubkos11/titanic-predictor.git)
    cd titanic-predictor
    ```
2.  Install the required libraries:
    ```bash
    pip install -r requirements.txt
    ```
3.  Open and run the `visualizations.ipynb` notebook in a Jupyter environment.

## ğŸ› ï¸ Technologies

* Python
* Pandas & NumPy
* Scikit-learn
* XGBoost
* Optuna
* Matplotlib & Seaborn
* Matplotlib & Seaborn
* Jupyter Notebook
