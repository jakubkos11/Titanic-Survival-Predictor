{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3136,"databundleVersionId":26502,"sourceType":"competition"}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport optuna\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\n\nclass Config:\n    \"\"\"Stores all configurations for the model.\"\"\"\n    FEATURES_TO_USE = [\n        'Pclass', 'Sex', 'FamilySize', 'IsAlone', 'Embarked',\n        'Title', 'AgeBin', 'FareBin', 'Deck'\n    ]\n\n    # Set of best parameters previously found by optuna\n    BEST_PARAMS = {\n        'n_estimators': 312,\n        'max_depth': 11,\n        'min_samples_leaf': 2,\n        'min_samples_split': 5,\n        'max_features': 'sqrt'\n    }\n    # Main toggler to decide whether we want to use optuna\n    TUNE_HYPERPARAMETERS = False\n    \n    # Number of trials for Optuna to run\n    N_TRIALS_OPTUNA = 50\n    \nclass RandomForestPredictor:\n    \"\"\"Implements complete pipeline for training and predicting model.\"\"\"\n    def __init__(self, config):\n        \"\"\"Initialize model.\"\"\"\n        self.config = config\n        self.preprocessor = None\n        self.model = None\n\n    def _clean_data(self, data):\n        \"\"\"Fill the missing values in columns.\"\"\"\n        df = data.copy()\n        # Fill with the median age grouped by Pclass and Sex for more accuracy.\n        df['Age'] = df.groupby(['Pclass', 'Sex'])['Age'].transform(lambda x: x.fillna(x.median()))\n    \n        # In case any group was entirely NaN, fill remaining with the global median\n        if df['Age'].isnull().any():\n            df['Age'] = df['Age'].fillna(df['Age'].median())\n    \n        # Fill missing values with the median.\n        df['Fare'] = df['Fare'].fillna(df['Fare'].median())\n    \n        # Fill the missing values with the most common port.\n        df['Embarked'] = df['Embarked'].fillna(df['Embarked'].mode()[0])\n        return df\n        \n    def _apply_feature_engineering(self, data):\n        \"\"\"Add custom engineering features to improve predictive power of model.\"\"\"\n        df = data.copy()\n        df['FamilySize'] = df['Parch'] + df['SibSp'] + 1\n        df['IsAlone'] = (df['FamilySize'] == 1).astype(int)\n        df['Title'] = data['Name'].str.extract(' ([A-Za-z]+)\\.', expand = False)\n        \n        # Simplify titles into a few main categories\n        common_titles = ['Mr', 'Miss', 'Mrs', 'Master']\n        df['Title'] = df['Title'].apply(lambda x: x if x in common_titles else 'Other')\n\n        df['Deck'] = df['Cabin'].str[0].fillna('M')\n        df['AgeBin'] = pd.cut(df['Age'], bins = [0, 18, 60, 100], labels = ['Child', 'Adult', 'Senior'])\n        df['FareBin'] = pd.qcut(df['Fare'], 4, labels = ['Low', 'Medium', 'High', 'VeryHigh'])\n        return df\n\n    def _build_preprocessor(self, X):\n        \"\"\"Builds preprocessing pipeline for numeric and categorical data.\"\"\"\n        # Select only the features we intend to use in the model\n        X = X[self.config.FEATURES_TO_USE]\n\n        # Automatically find numeric and categorical columns based on their data types\n        num_features = X.select_dtypes(include = np.number).columns.tolist()\n        cat_features = X.select_dtypes(include = ['object', 'category']).columns.tolist()\n        \n        num_pipeline = Pipeline([\n            ('imputer', SimpleImputer(strategy = 'median')),\n            ('scaler', StandardScaler())\n        ])\n        cat_pipeline = Pipeline([\n            ('imputer', SimpleImputer(strategy = 'most_frequent')),\n            ('onehot', OneHotEncoder(handle_unknown = 'ignore', sparse_output = False))\n        ])\n        preprocessor = ColumnTransformer([\n            ('num', num_pipeline, num_features),\n            ('cat', cat_pipeline, cat_features)\n        ])\n        return preprocessor\n\n    def _tune_with_optuna(self, X, y):\n        \"\"\"Finds the best hyperparameters using Optuna.\"\"\"\n        def objective(trial):\n            \"\"\"Define the search space for algorithm.\"\"\"\n            params = {\n                'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n                'max_depth': trial.suggest_int('max_depth', 5, 20),\n                'min_samples_leaf': trial.suggest_int('min_samples_leaf', 2, 15),\n                'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),\n                'max_features': trial.suggest_categorical('max_features', ['sqrt', 'log2']),\n            }\n            model = RandomForestClassifier(**params, random_state = 1, n_jobs = -1)\n            pipeline = Pipeline(steps = [\n                ('preprocessor', self.preprocessor),\n                ('classifier', model)\n            ])\n            # Use Stratified Fold for more accurate cross validation\n            strfold = StratifiedKFold(n_splits = 5, shuffle = True, random_state = 1)\n            score = cross_val_score(pipeline, X, y, cv = strfold, scoring = 'accuracy').mean()\n            return score\n\n        study = optuna.create_study(direction = 'maximize')\n        study.optimize(objective, n_trials = self.config.N_TRIALS_OPTUNA, show_progress_bar = True)\n        \n        return study.best_params\n\n    def train(self, X, y):\n        \"\"\"Trains a logistic regression model using gradient descent.\"\"\"\n        # Build the preprocessor\n        self.preprocessor = self._build_preprocessor(X)\n\n        params = self.config.BEST_PARAMS\n        if self.config.TUNE_HYPERPARAMETERS:\n            params = self._tune_with_optuna(X, y)\n        \n        # Create the final pipeline with the best parameters\n        self.model = Pipeline(steps = [\n            ('preprocessor', self.preprocessor),\n            ('classifier', RandomForestClassifier(**params, random_state = 1, n_jobs = -1))\n        ])\n        # Train model on all available data\n        self.model.fit(X, y)\n        \n        # Show performance metrics\n        self._evaluate(X, y, params)\n        \n    def _evaluate(self, X, y, best_params):\n        \"\"\"Prints performance metrics on a hold-out validation set for an honest estimate.\"\"\"\n        X_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.2, random_state = 1, stratify = y)\n        \n        temp_model = Pipeline(steps = [\n            ('preprocessor', self._build_preprocessor(X_train)),\n            ('classifier', RandomForestClassifier(**best_params, random_state = 1, n_jobs = -1))\n        ])\n        temp_model.fit(X_train, y_train)\n\n        train_preds = temp_model.predict(X_train)\n        train_accuracy = accuracy_score(y_train, train_preds)\n        val_preds = temp_model.predict(X_val)\n        val_accuracy = accuracy_score(y_val, val_preds)\n\n        print(\"\\n--- Model Performance Estimate ---\")\n        print(f\"Hyperparameters Used: {best_params}\")\n        print(f\"Training Set Accuracy: {train_accuracy:.4f}\")\n        print(f\"Validation Set Accuracy: {val_accuracy:.4f}\")\n    \n    def predict(self, X):\n        \"\"\"Gives final predictions for test data.\"\"\"\n        return self.model.predict(X)\n\n    def run(self):\n        \"\"\"Runs the entire pipeline from loading data to submission.\"\"\"\n        # Load data\n        train_df = pd.read_csv('/kaggle/input/titanic/train.csv')\n        test_df = pd.read_csv('/kaggle/input/titanic/test.csv')\n\n        # Separate features and target\n        X_train = train_df.drop('Survived', axis = 1)\n        y_train = train_df['Survived']\n        X_test = test_df.copy()\n\n        # Clean data\n        X_train = self._clean_data(X_train)\n        X_test = self._clean_data(X_test)\n\n        # Apply feature engineering\n        X_train = self._apply_feature_engineering(X_train)\n        X_test = self._apply_feature_engineering(X_test)\n\n        # Train model\n        self.train(X_train, y_train)\n\n        # Make predictions on the test set\n        predictions = self.predict(X_test)\n        \n        # Generate the submission file\n        submission = pd.DataFrame({'PassengerId': test_df['PassengerId'], 'Survived': predictions})\n        submission.to_csv('submission.csv', index = False)\n\n# Main function\nif __name__ == '__main__':\n    config = Config()\n    predictor = RandomForestPredictor(config = config)\n    predictor.run()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-22T18:11:34.722990Z","iopub.execute_input":"2025-09-22T18:11:34.723316Z","iopub.status.idle":"2025-09-22T18:11:37.159524Z","shell.execute_reply.started":"2025-09-22T18:11:34.723293Z","shell.execute_reply":"2025-09-22T18:11:37.158636Z"}},"outputs":[{"name":"stdout","text":"\n--- Model Performance Estimate ---\nHyperparameters Used: {'n_estimators': 312, 'max_depth': 11, 'min_samples_leaf': 2, 'min_samples_split': 5, 'max_features': 'sqrt'}\nTraining Set Accuracy: 0.8708\nValidation Set Accuracy: 0.8212\n","output_type":"stream"}],"execution_count":14}]}